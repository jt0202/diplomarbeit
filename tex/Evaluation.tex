\chapter{Evaluation}\label{sec:eval}

In the previous sections, we proved the correctness of the algorithms to check the soundness and completeness of datalog reasoning results. Now, we are interested in the practicability of these algorithms on actual data. We combined these algorithms into a command line tool that takes a file consisting of the problem and the certificates and tells us whether the result is correct according to the certificate. 

\section{Input format}

The input format is JSON-based because Lean offers already direct support for JSON. Similarly, as we were able to derive decidable equality or the inhabited class, we can also derive functions that convert Lean objects to JSON objects or try to create a Lean object from a JSON object.

We can define mock terms similar to \term but with the variables and constants as simple strings. Lean knows how to read and write strings into JSON hence we can derive the JSON methods.

\begin{lstlisting}
inductive (.\mockTerm.)
| constant: String → mockTerm
| variable: String → mockTerm
deriving DecidableEq, Lean.FromJson, Lean.ToJson, Repr
\end{lstlisting}

Using this type we can similarly as to \atom define \mockAtom. In contrast to real atoms, we do not require a proof that the number of terms matches the arity of the predicate symbol as encoding such a proof is difficult and we have no information about the arity. The symbol is again just a string.

\begin{lstlisting}
structure (.\mockAtom.) where
  (symbol: String)
  (terms: List mockTerm)
deriving DecidableEq, Lean.FromJson, Lean.ToJson, Repr
\end{lstlisting}

Mock atoms form mock rules similarly as atoms form rules and a program is simply a list of mockRules. Lists are a basic feature of the JSON decoder which allows us to get the from a JSON file. 

\begin{example}
    The program 

    \begin{equation}
        \begin{split}
            &P = \{  \\
            &T(?x,?y) \leftarrow E(?x,?y), Q(a).\\
            \}
        \end{split}
    \end{equation}

    is represented as following in JSON:

    \begin{lstlisting}
        "program": [
        {
            "head": {
                "symbol": "T",
                "terms": [
                    {
                        "variable": "?x"
                    },
                    {
                        "variable": "?y"
                    }
                ]
            },
            "body": [
                {
                    "symbol": "E",
                    "terms": [
                        {
                            "variable": "?x"
                        },
                        {
                            "variable": "?y"
                        }
                    ]
                },
                {
                    "symbol": "Q",
                    "terms": [
                        {
                            "constant": "a"
                        }
                    ]
                }

            ]
        }]
    \end{lstlisting}
\end{example}

Afterwards, we go through the program twice. In the first run, we collect all the predicate symbols and their arities (\parsingArityHelper) into a list and report an error if a predicate symbol is used in multiple atoms with different amounts of terms. Using the list we can construct a signature (\parsingSignature). We use for constants and variables simply the set of strings as types and for predicate symbols the subset of strings that occurred as symbols. These choices allow us to directly inherit the requirements for the signature elements such as decidable equality or hashability. 

In the second run, we then transform every mock object into the corresponding datalog object of the previously created signature. 

The second part is the input file is either a list of trees or a graph. For each of the objects, we define again mock objects and transform them into the corresponding datalog objects after we have transformed the program.

\begin{lstlisting}
inductive jsonTree (A: Type)
| node (label: A) (children: List (jsonTree A))
deriving Lean.FromJson, Lean.ToJson

-- graph validation
structure mockEdge where
  (vertex: mockAtom)
  (successors: List (mockAtom))
deriving DecidableEq, Lean.FromJson, Lean.ToJson

structure mockGraph where
  (edges: List mockEdge)
deriving Lean.FromJson, Lean.ToJson
\end{lstlisting}

Additionally, there are two command line options that can be set. Firstly, the option \textit{-g} specifies that the input file uses a graph instead of a list of trees which is the default option. Secondly, we tell the program with \textit{-c} to also use the \modelChecker to check for completeness.

The file does not include a database yet because these databases are often very large in practice which requires more work to replicate in Lean. Additionally, we would need to prove that the database is correctly read from the input. All evaluations are done with a mock database which assumes that any leaf of a tree or graph is in the database and that the database is always contained in the model during the model checking.


\section{Results}

In this section, we want to evaluate the program in practice. We are interested in the feasibility of our approach to verify proof trees and to check a complete result. Additionally, we want to compare the tree verification with the graph-based verification.

We will use three kinds of datalog programs to test our approach:

\begin{enumerate}
    \item We will use the transitive closure programs we considered in this work multiple times. The transitive closure is often used as a subroutine in recursive datalog programs and thus an important benchmark. The exact program uses the transitive predicate once in the body and is depicted below:
    \begin{align*}
        \begin{split}
            Trans(?x, ?y) &\leftarrow Edge(?x, ?y). \\
            Trans(?x, ?z) &\leftarrow Edge(?x, ?y),  Trans(?y, ?z). 
        \end{split}
    \end{align*}

    This program can be used with different directed graphs and may be considered a reachability problem. We create the graphs in a Python script using networkx's \lstinline|gnm_random_graph| which uniformly selects a graph from the graphs for a given number of vertices and edges. We use different densities to test different graphs. The density describes the ratio between the number of edges and the maximal number of edges.

    \item We will reuse the program from \cref{ex:treeGraph} on chains of different lengths. We have theoretically analyzed that the proof trees are exponentially larger than the corresponding proof graphs and want to see if the algorithms also respect that in practice.
    
    \item Finally, we will use a large example from practice. The problem deals with reasoning in the OWL EL profile of the web ontology language OWL whose transformation to datalog is described in \cite{ELK}. As the database, we reuse the medical ontology GALEN which is also used in the original paper. The ontology is preprocessed in CSV files and leads to around 2.4 million derived atoms.
\end{enumerate}

The program and all scripts to generate the example data and run the experiments available in the \href{https://github.com/knowsys/CertifyingDatalog/tree/diplomarbeit/Examples/diplomarbeit}{diplomarbeit subfolder} of the examples in the repository.

We use the datalog engine Nemo\cite{Nemo} to evaluate these examples. Nemo allows us to specify multiple facts as a file and ask for their derivations. This is returned in a machine-readable JSON format which allows us to transform this and the program into the input file format described in the previous section.

Any other datalog engine that returns proof trees or graphs can also be used as long as such a program exists to convert it into the input format. This would also be possible with Soufflé\cite{Souffle} but the proof trees there are only given in an ASCII art style which complicates the parsing.

The experiments run on a modern laptop with an AMD Ryzen 7 processor, 8 GB of RAM using Ubuntu in the Windows subsystem for Linux.
We will measure the processor time using Python's time module in three categories. Firstly, we measure the time our tool takes to verify the solution. Secondly, we measure the time Nemo takes to calculate the solution in our scenario. As there is to the best of our knowledge currently no alternative program capable of verifying datalog reasoning results this is the only other time available for comparison. We believe that the verification can take a bit longer than the computation but it should not take hours for a task that Nemo can solve in seconds. 
These two times ignore however an important aspect. We need to create first the input file that contains the program and the proof trees or proof graph. This is the third time we measure, the preparation time. This consists of the time that Nemo takes to create the trace which takes longer than just calculating the solution and the time we need to transform the trace into our JSON format which is done by a Python script.

We repeat each measurement five times and display in each table the arithmetic mean and standard derivation in seconds for each time calculated by Python's pandas library.


\begin{table}
    \centering
    \caption{Comparision between tree validation and a completeness check in scenario 1}       
    \label{table:complScen1}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|l|rr|rr|rr}
        &  & \multicolumn{2}{r}{\textbf{Nemo time [s]}} & \multicolumn{2}{r}{\textbf{Preparation time [s]}} & \multicolumn{2}{r}{\textbf{Validation time [s]}} \\
        &  & mean & std & mean & std & mean & std \\
       \textbf{Density} & \textbf{Completeness} &  &  &  &  &  &  \\
       \hline
       \multirow[c]{2}{*}{0.01} & False & 0.03 & 0.01 & 0.84 & 0.30 & 0.04 & 0.01 \\
        & True & 0.03 & 0.01 & 0.84 & 0.30 & 0.11 & 0.06 \\
        \hline
       \multirow[c]{2}{*}{0.05} & False & 0.05 & 0.00 & 239.50 & 17.18 & 0.45 & 0.04 \\
        & True & 0.05 & 0.00 & 239.50 & 17.18 & 26.24 & 1.64 \\
        \hline
       \multirow[c]{2}{*}{0.1} & False & 0.06 & 0.00 & 180.94 & 1.41 & 0.42 & 0.03 \\
        & True & 0.06 & 0.00 & 180.94 & 1.41 & 39.30 & 3.53 \\
        \hline
       \multirow[c]{2}{*}{0.3} & False & 0.10 & 0.01 & 171.27 & 1.54 & 0.34 & 0.01 \\
        & True & 0.10 & 0.01 & 171.27 & 1.54 & 104.95 & 3.14 \\
        \hline
       \multirow[c]{2}{*}{0.5} & False & 0.16 & 0.01 & 193.46 & 1.58 & 0.31 & 0.00 \\
        & True & 0.16 & 0.01 & 193.46 & 1.58 & 193.41 & 4.94 \\
       \end{tabular}
    }
\end{table}

In scenario (1), depicted in \cref{table:complScen1}, we created a random graph with the given density and asked Nemo to explain all derivations. The number of derivations is up to 10000 facts in the case of a density of 0.3 or 0.5. We use the tree-based input and want to compare the time it takes to verify all proof trees with the time that we require to additionally perform a completeness check. We see that validating all trees is fast while being a bit larger than the computation times by Nemo. In contrast to that, a completeness check takes in most cases much more time than just verifying all trees. The time for a completeness check increases with the density of the graphs as the graph has more edges with increasing density which increases the number of edges we check for substitutions. In contrast to that the validation time without a completeness check does not increase with the density. We propose that with the increasing densities, there are more edges which lead to shallower trees which can be faster validated even if their number increases.

We see in this scenario that it is definitely feasible to validate all trees of the example as the time is roughly comparable to Nemo's time. We did not encounter an invalid tree or an invalid model. Next, we want to compare the performance of the tree-based input format with the graph-based input format. We do not use the completeness check this time as we have seen in the previous results that the completeness check takes the majority of the time while not depending on the particular input format.

In \cref{table:graphTree1} see for each density the times we require for trees and the graph. We see that the validation of the graph is a bit faster than validating the trees in each case but still in the same area. Graphs are especially faster for the densities of 0.1 and 0.05 where the trees need the most nodes as there are many nodes connected while still needing long paths. If the paths are shorted with a higher density, the proof trees are smaller and the advantage diminishes. The main advantage of the graph-based input is however seen in the preparation times. Transforming Nemo's trace output into a directed graph is much faster than transforming the trace output into a list of trees because Nemo's trace output is graph-like. Therefore graphs seem preferable in this use-case as we cannot validate the results without preparing them.

\begin{table}
    \centering
    \caption{Comparision between trees and graphs in Scenario 1}
    \label{table:graphTree1}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|l|rr|rr|rr}
        &  & \multicolumn{2}{r}{\textbf{Number of nodes}} & \multicolumn{2}{r}{\textbf{Preparation time [s]}} & \multicolumn{2}{r}{\textbf{Validation time [s]}} \\
        &  & mean & std & mean & std & mean & std \\
       \textbf{Density} & \textbf{Type} &  &  &  &  &  &  \\
       \hline
       \multirow[c]{2}{*}{0.01} & graph & 606.20 & 117.57 & 0.19 & 0.06 & 0.03 & 0.00 \\
        & tree & 3858.40 & 1763.22 & 0.84 & 0.30 & 0.04 & 0.01 \\
        \hline
        \multirow[c]{2}{*}{0.05} & graph & 10295.40 & 157.80 & 18.07 & 1.55 & 0.24 & 0.02 \\
        & tree & 59235.60 & 1374.27 & 239.50 & 17.18 & 0.45 & 0.04 \\
        \hline
        \multirow[c]{2}{*}{0.1} & graph & 10990.00 & 0.00 & 17.68 & 0.21 & 0.27 & 0.04 \\
        & tree & 44750.00 & 126.88 & 180.94 & 1.41 & 0.42 & 0.03 \\
        \hline
        \multirow[c]{2}{*}{0.3} & graph & 12970.00 & 0.00 & 19.06 & 0.24 & 0.28 & 0.01 \\
        & tree & 34061.60 & 1.67 & 171.27 & 1.54 & 0.34 & 0.01 \\
        \hline
        \multirow[c]{2}{*}{0.5} & graph & 14950.00 & 0.00 & 21.12 & 0.22 & 0.27 & 0.01 \\
        & tree & 30100.00 & 0.00 & 193.46 & 1.58 & 0.31 & 0.00 \\
       \end{tabular}
    }
\end{table}


\begin{table}
\centering
\caption{Results for scenario 2}
\label{table:res2}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l|r|rr|rr|rr}
    &  & \textbf{Number of nodes} & \multicolumn{2}{r}{\textbf{Nemo time [s]}} & \multicolumn{2}{r}{\textbf{Preparation time [s]}} & \multicolumn{2}{r}{\textbf{Validation time [s]}} \\
    &  & mean & mean & std & mean & std & mean & std \\
    \textbf{Type} & \textbf{Size} &  &  &  &  &  &  &  \\
    \hline
   \multirow[c]{3}{*}{graph} & 10 & 18.00 & 0.03 & 0.01 & 0.03 & 0.01 & 0.05 & 0.06 \\
    & 15 & 28.00 & 0.02 & 0.00 & 0.03 & 0.00 & 0.02 & 0.00 \\
    & 20 & 38.00 & 0.02 & 0.00 & 0.03 & 0.00 & 0.03 & 0.00 \\
   \multirow[c]{3}{*}{tree} & 10 & 1022.00 & 0.03 & 0.01 & 0.07 & 0.01 & 0.03 & 0.00 \\
    & 15 & 32766.00 & 0.02 & 0.00 & 2.71 & 0.09 & 0.16 & 0.00 \\
    & 20 & 1048574.00 & 0.02 & 0.00 & 207.66 & 3.98 & 4.38 & 0.20 \\
\end{tabular}
}
\end{table}

In scenario (2), depicted in \cref{table:res2}, the advantage of graphs over trees is seen clearly even in the validation time. For chains of the length 15 and 20 the tree-based validation takes significantly more time while the graph-based validation is still comparable to Nemo's computation time. Additionally, the preparation for graphs is again much faster than the preparation time for the tree-based input format. This is because Nemo's trace looks graph-like so we only have to transform the atoms into the JSON format. Creating a tree requires us to first build the graph and then explore all paths. When trying to evaluate the scenario for chains of the length 25 the preparation failed on this setup whereas it was still possible to verify this using graphs with a comparable time in the previous graph experiments.

    \begin{table}
    \centering
    \caption{Results for scenario 3}
    \label{res:scenario3}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|l|rr|rr|rr}
        &  & \multicolumn{2}{r}{\textbf{Number of nodes}} & \multicolumn{2}{r}{\textbf{Preparation time [s]}} & \multicolumn{2}{r}{\textbf{Validation time [s]}} \\
        &  & mean & std & mean & std & mean & std \\
       \textbf{Number of atoms} & \textbf{Type} &  &  &  &  &  &  \\
       \hline
       \multirow[c]{2}{*}{1} & graph & 18.60 & 7.20 & 8.90 & 0.06 & 0.02 & 0.00 \\
        & tree & 19.00 & 7.94 & 8.93 & 0.02 & 0.03 & 0.00 \\
        \hline

       \multirow[c]{2}{*}{10} & graph & 230.80 & 39.33 & 9.10 & 0.11 & 0.03 & 0.00 \\
        & tree & 286.00 & 46.03 & 9.13 & 0.09 & 0.03 & 0.00 \\
        \hline

       \multirow[c]{2}{*}{100} & graph & 1876.00 & 148.04 & 11.04 & 0.44 & 0.05 & 0.00 \\
        & tree & 2835.40 & 373.87 & 11.46 & 0.49 & 0.04 & 0.00 \\
        \hline

       \multirow[c]{2}{*}{1000} & graph & 12813.40 & 187.66 & 38.88 & 1.18 & 0.20 & 0.01 \\
        & tree & 28159.80 & 399.11 & 68.04 & 3.03 & 0.18 & 0.00 \\
        \hline

       \multirow[c]{2}{*}{10000} & graph & 85851.20 & 484.08 & 1301.97 & 210.82 & 1.60 & 0.16 \\
        & tree & 286027.60 & 2172.81 & 4101.78 & 319.53 & 1.91 & 0.17 \\
       \end{tabular}
    }
    \end{table}


In scenario (3), depicted in \cref{res:scenario3}, we test the verification on another data set. This data set is a fixed ontology GALEN. Thus we change the size by increasing the number of atoms we ask traces for proofs which is depicted in the left-most column. The results of the program are computed by Nemo in around 10s on the setup used in this section. We see again that verification is very fast for trees and graphs. The difference in times between the graph and tree input is not as significant as in scenario (2). We see in the case of asking for one fact the number of nodes in the tree and graph are almost equal so atoms are not as often reused in this scenario as in the previous scenario. The difference in node numbers is larger in the case of multiple facts as multiple proof trees will use the same fact. The preparation time for graphs is in this scenario for larger instances of at least 1000 atoms much faster than for trees. We also see that a completeness check is futile here as we need a certificate for every atom while creating certificates for 10000 atoms in the graph format already takes around 20 minutes. Creating certificates for all 2.4 million facts was not possible in this setup. 


In summary, we see that validating a proof tree or proof graph is possible in practice in most cases. The graph approach is significantly faster in the worst case but in most user-defined cases they are similar. The input format for the validation is however faster created for graphs due to the shape of Nemo's trace output. If the output is more tree-like then this might change. Therefore we believe that the graph input is better to use with Nemo. We also see that completeness is possible to check for small instances but takes much longer than just verifying the soundness or calculating the result itself. Our completeness check is similar to the computation that is done by a datalog engine but without the typical explanation. Therefore this behaviour is expected in larger problems. 
Additionally, we have never detected an error in any of Nemo's output. We note that on inputs that were built to provoke errors, these errors were found as aspected by our correctness proofs.