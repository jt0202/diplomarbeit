\chapter{Preliminaries}\label{sec:prelim}


In this section, we introduce concepts that we are going to use in future sections. We start by defining datalog, which we will formalize later. After that, we review the concept of certifying algorithms and finally, we introduce Lean.


\section{Datalog}
Datalog is a logic programming language and can be syntactically considered as a subset of Prolog. An introduction to datalog and other query languages can be found in \cite{alice}, which we recall next. We assume the reader to be familar with first-order logic as introduced in e.g. \cite{logic}.

In order to write datalog rules, we require the existence of three at countable, possibly infinite sets $V, C$ and $R$, where $V$ represents variables, $C$ represents constants and $R$ represents relation symbols. Every relation symbol $r$ in $R$ has an arity $ar(r) \in \mathbb{N}$. In this work we assume that zero is a natural number.
We use the following conventions in this work: For $R$ we use strings that start with a capital letter, for $V$ strings that start with a question mark and for $C$ any other non-empty string.

\begin{example}\label{ex:basicSets}
We consider have the variables $V= \{?x,?y,?z\}$ and constants $C= \{1,...,7\}$. The predicate symbols are the binary predicates (i.e. they have an arity of two) $S$ and $E$ and the nullary (i.e. arity zero) predicate $Q$.
\end{example}

In the language of first-order logic as in \cite{logic}, $C$ and $P$ build the signature of the language we are going to construct. We note that this fragment of first-order logic does not use function terms.

The following definitions are adapted from logic for the signature above.

A \textit{term} is either a variable or a constant. In order to differentiate meaningfully between them, we require that $C \cap V = \emptyset$ so that no symbol is both a variable and a constant. As long as both $C$ and $V$ are finite, we will only have a finite number of terms. This is in contrast to general first-order logic or logic programming where function symbols can lead to an infinite amount of terms even if the signature is finite.

\begin{example}
    Examples of terms in our language from \cref{ex:basicSets} are $?z$, $2$, $5$ or $?x$.

    A counterexample would be $word$ or $A$ as those elements do not occur in either $C$ or $V$.
\end{example}

An \textit{atom} is an expression of the form $A(t_1,...,t_n)$ for $n \in \mathbb{N}$ where $A$ is a relation symbol ($A \in P$) and $t_1,..,t_n$ are terms. Additionally, we require that $n$ is the arity of $A$, i.e. $ar(A) = n$.

\begin{example}
    Examples for atoms using the sets from \cref{ex:basicSets} are here: $Q(), S(?x,?y), E(2,3)$ or $E(?x, 7)$. In every atom, the number of terms matches the arity of the predicate symbol. Note that it is allowed to mix variables and constants in the terms of an atom.

    A first counterexample would be $Q(a)$ because the arity of the symbol $Q$ does not match the number of terms. 
    Another counterexample would be $S(E(2,3),4)$ because $ E(2,3)$ is not a term.
\end{example}

In the example, some atoms used only constants in their terms, while others included variables. We call an atom a \textit{ground atom} if it is variable-free. The variables used in an atom $A(t_1 \dots t_n)$ are formally the following set: \[Vars(A(t_1,...,t_n)) = \{t_i \mid t_i \in V\}\].

\begin{example}
    In the previous example $Q()$ and $E(2,3)$ are ground atoms. The first does not have any terms at all, whereas the other only uses constants.

    The other atoms all contain at least the variable $?x$ and are thus no ground atoms.
\end{example}

A \textit{rule} is an expression of the form $H \leftarrow B_1 , ... , B_n$ for atoms $H$ and $B_i$ for $n \in \mathbb{N}$. We call $H$ the \textit{head} of the rule and $B_1,..., B_n$ the \textit{body} of the rule and define the functions $head(r) = H$ and $body(r) = \{B_i \mid i \in \{1,\dots n\}\}$. \textit{Facts} are rules where $n$ is zero.

We can generalize the $Vars$ function to rules and use this to define \textit{ground rules}. For a rule $r = H \leftarrow B_1 , ... ,B_n$, we define $Vars(r)$ to be the union of the variables occuring in the head and the body: \[Vars(r) = Vars(H) \cup \bigcup_{i \in \{1,..., n\}} Vars(B_i)\] and call $r$ a ground rule if $Vars(r) = \emptyset$. This means that a rule is a ground rule if the head and every atom in the body is a ground atom.

\begin{example}
    Examples for rules are $E(1,2) \leftarrow $, $Q() \leftarrow E(?x,?y)$ or $T(?x,?z) \leftarrow E(?x, ?y), T(?y,?z)$.

    $E(1,2) \leftarrow $ is both a fact and a ground rule, but not every fact is a ground rule as for example $E(?x, ?x) \leftarrow$
\end{example}

A \textit{program} $P$ is a finite set of rules. 

\begin{example}\label{prelim:Program}
    We consider the program $P $ with the following rules:

    \begin{align*}
    \begin{split}
    E(1,2) &\leftarrow \\ E(1,3) &\leftarrow \\ E(3,5) &\leftarrow \\ E(4,6) &\leftarrow \\E(4,7) &\leftarrow\\\end{split} 
    &&
    \begin{split}
        Q() &\leftarrow \\ T(?x,?y) &\leftarrow E(?x,?y) \\ T(?x,?x) &\leftarrow \\ T(?x, ?z) &\leftarrow T(?x,?y), T(?y, ?z)
    \end{split}
    \end{align*}
    $P$ contains both facts and other rules.
\end{example}

With this, we have introduced all the needed elements for the syntax of datalog, but we have not yet considered the semantics of datalog. What is the program $P$ supposed to express?

We have noted the similarities between datalog and first-order logic when defining the syntax of datalog. Semantically, we can view rules as elements of the universally quantified Horn-fragment of first-order logic. In the logic programming style the universal quantifiers are usually omitted and the implications are written from right to left. As a theory is a set of formulae we can view the program as a theory consisting of its rules.


\begin{example}
    The equivalent sentence for the fact $E(1,2) \leftarrow$ is 
    
    \[E(1,2)\]
    
    
    and for the rule $T(?x, ?z) \leftarrow T(?x,?y), T(?y, ?z)$ is 
    
    \[\forall ?x,?y,?z. T(?x,?y) \land T(?y, ?z) \rightarrow T(?x, ?z)\]
\end{example}

The semantics of a first-order theory or sentence are its models. Unfortunately, a program may have multiple models as the following example shows for the theory for $P$.


\begin{example}\label{prelim:models}
    Any fact must be true in a model, but the implications allow us more freedom. As both $E$ and $T$ are binary predicates, we can represent them as a graph. We use dashed edges, when both $E$ and $T$ are present, solid edges for only $T$ and dotted edges when only $E$ is present.

    \begin{figure}
        \centering
        \begin{subfigure}{0.45\linewidth}
        \begin{tikzpicture}[, every node/.style={circle, draw, minimum size=8mm}]
            % Nodes
            \node (1) at (90:2) {1};
            \node (2) at (45:2) {2};
            \node (3) at (0:2) {3};
            \node (4) at (-45:2) {4};
            \node (5) at (-90:2) {5};
            \node (6) at (-135:2) {6};
            \node (7) at (180:2) {7};

            %self loop
            \foreach \i in {1,...,3} {
                \draw[->] (\i) edge [loop above] (\i);
            }
            \foreach \i in {4,...,7} {
                \draw[->] (\i) edge [loop below] (\i);
            }

            % Edges
            \draw[->,dashed] (1) -- (2);
            \draw[->,dashed] (1) -- (3);
            \draw[->,dashed] (3) -- (5);
            \draw[->,dashed] (4) -- (7);
            \draw[->,dashed] (4) -- (7);
            \draw[->] (1) -- (5);
        \end{tikzpicture}
        \caption{First model describing the reflexive-transitive closure}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \begin{tikzpicture}[, every node/.style={circle, draw, minimum size=8mm}]
            % Nodes
            \node (1) at (90:2) {1};
            \node (2) at (45:2) {2};
            \node (3) at (0:2) {3};
            \node (4) at (-45:2) {4};
            \node (5) at (-90:2) {5};
            \node (6) at (-135:2) {6};
            \node (7) at (180:2) {7};

            %self loop
            \foreach \i in {1,...,3} {
                \draw[->] (\i) edge [loop above] (\i);
            }
            \foreach \i in {4,...,7} {
                \draw[->] (\i) edge [loop below] (\i);
            }

            % Edges
            \draw[->,dashed] (1) -- (2);
            \draw[->,dashed] (1) -- (3);
            \draw[->,dashed] (3) -- (5);
            \draw[->,dashed] (4) -- (6);
            \draw[->,dashed] (4) -- (7);
            \draw[->,dashed] (2) -- (3);
            \draw[->] (1) -- (5);
            \draw[->, dotted] (6) -- (7);
            \draw[->, dotted] (7) -- (1);
            \draw[->,dashed] (4) -- (1);
        \end{tikzpicture}
        \caption{Second model: Extension of (a)}
    \end{subfigure}
    \caption{Models for the program from \cref{prelim:Program}. The encoding is explained in \cref{prelim:models}}
    \label{prelim:modelsPict}
    \end{figure}
    
    The first model expresses $T$ as the reflexive transitive closure of the input $E$, whereas the second model expands the first model and adds more additional atoms to the previous model.
\end{example}

There are multiple models but we already see in the example there one model is a subset of the other. Such a model always exists in datalog and is called the least model. This model is of interest as the atoms that a true in this model are true in every model

We call a finite set of ground atoms an \textit{interpretation}. An interpretation represents all true atoms. We want to define the model property from first-order logic for this definition and show then that there exists a least model according to the subset relation.

Recall from first-order logic that $\forall x. \phi(x)$ holds in a structure $\mathcal{A}$ if for any element $a \in \mathcal{A}$ we have that $\phi(a)$ holds. Therefore we could just create ground rules by replacing variables with all possible constants.
Formally, this is defined using \textit{groundings} or \textit{instantiations} which are functions that map variables to constants. We can apply a grounding $g$ to an atom by replacing every variable $v$ in the terms by $g(v)$ and apply $g$ to a rule by applying it to the head and every atom in the body. At the end of this, we have replaced every variable by a constant and have gained a ground rule. 
The ground program $ground(P)$ of a program $P$ is the set of all ground rules that are the result of applying some grounding to a rule from $P$.

We call a ground rule $r$ \textit{true} in an interpretation $I$ if whenever $body(r)$ is a subset of $I$, then also $head(r)$ is in $I$.  We call an interpretation $I$ a \textit{model} of a program $P$ if every rule of $ground(P)$ is true in $I$.

So now we have defined models and can define the least model as well. We still need to show that the least model exists for which the following lemma is helpful.

\begin{lemma}\label{lem:ModelIntersection}
Let $M_1, M_2$ be two models of a program $P$. Then also $M_1 \cap M_2$ is a model of $P$.
\end{lemma}

Therefore the intersection of all models is a model as well and due to the properties of the intersection it is least according to the subset relation. 

In total, we call \[\bigcap_{\text{$M$ is model of $P$}} M\] the \textit{least model} of $P$ and refer to this characterization as the \textit{model-theoretic semantics} of $P$.

\begin{example}
    In this example, the first model in of \cref{prelim:modelsPict} is actually the least model and the second model is just some other model.

    Therefore the semantic of this program is the reflexive-transitive closure of $E$ if $Q()$ is given.
    If $Q()$ is not given then it is only the reflexive closure.

    Additionally, we see that the rules for $T$ encode general rules whereas the other rules only encode a specific input. We might want to reuse these rules for many different instances of $E$, but so far we also need to write a new program.
\end{example}

The example raises questions about the reusability of a program. Additionally, we talked in the introduction about database queries but never talked about databases. 

We consider a \textit{database} as a finite set of ground atoms similar to an interpretation. What is now the semantics of a program $P$ and a database $d$? We simply add every element of $d$ as a fact to $P$ and reuse the previous semantics. We can also move all the facts that are ground rules in the program into the database and gain a more reusable program.
An alternative model definition for a pair of $P$ and $d$ is therefore: An interpretation $I$ is a model for $P$ and $d$ if every ground atom from $d$ is in $I$ and every rule from $ground(P)$ is true in $I$. Then \cref{lem:ModelIntersection} holds again and we reuse the definition for the database and program case.

Both views are equivalent. We have shown how we can simulate a database by a program. We can simulate the program case with the program and database case by simply using an empty database.

We have now defined the semantics of datalog and shown a connection with databases. But this definition is not ideal. In order to find the semantics of a program we would need to check all interpretations and then intersect them all. This is computationally expensive. Is there a simpler way?

We can define a function that computes the least model in a more direct way. For this, we consider the case where only the program $P$ is present.

Consider an interpretation $I$. We call a ground atom $a$ an \textit{immediate consequence} of $I$ if there exists a ground rule $r$ in $ground(P)$ with the head $a$ and $body(r) \subseteq I$.

The immediate consequence operator $T_P$ computes all immediate consequences of an interpretation. 

\[T_P(I) = \{ a \mid \text {$a$ is an immediate consequence of $I$}\} \]

A fixed point of a function $f$ is an element $k$ such that $f(k) = k$. The repeated application of $T_P$ starting from $\emptyset$ yields a fixed point that is equal to the model-theoretic semantics of a program $P$. 

\begin{example}
    We consider again the program $P$ from \cref{prelim:Program}. 

    Due to the fact $E(1,3) \leftarrow$, $E(1,3)$ is an immediate consequence of $\emptyset$. Applying $T_P$ again, we have that $T(1,3)$ is an immediate consequence of $\{E(1,3)\}$ due to the rule $T(?x,?y) \leftarrow E(?x, ?y)$ with the grounding 
    
    \[
    g(v) =
    \begin{cases}
        1 & \text{if } v = ?x \\
        3 & \text{else}
    \end{cases}
    \]

    Applying this grounding to every fact, we gain an interpretation $I$ that contains $T(1,3)$ and $T(3,5)$. Then $T(1,5)$ is an immediate consequence of $I$ due to the rule $T(?x, ?z) \leftarrow T(?x,?y), T(?y, ?z)$ and the grounding 

    \[
    g'(v) =
    \begin{cases}
        1 & \text{if } v = ?x \\
        3 & \text{if} v = ?y \\
        5 & \text{else}
    \end{cases}
    \]

    We can continue this process until we find the fixed point.
\end{example}

The least fixed-point of $T_P$ is called the \textit{fixed-point semantics} of $P$ and is the basis for most implementations of datalog reasoners. 

Our goal is to explain why an atom is in the datalog result. The third important semantics of datalog helps here. 

A tree $t$ of whose vertices are labeled with ground atoms is a valid proof tree for a ground atom $a$ in a program $P$ and database $d$ if the following conditions hold:

\begin{enumerate}
    \item The root of $t$ is labelled by $a$
    \item For every node $n$ with the label $a$ and its children $l$ in $t$ one of the following two conditions holds: 
    \begin{enumerate}
        \item $a \leftarrow l_1, \dots ,l_n$ for $l_1,.., l_n$ being the labels of the elements in $l$ is a ground rule from $ground(P)$, or
        \item $n$ is a leaf and $a$ is in the database.
    \end{enumerate}
\end{enumerate}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
    \begin{tikzpicture}[, every node/.style={circle}]
        \node {T(1,5)}
            child {node {T(1,3)}
                child {node {E(1,3)}}
                }
            child {node {T(3,5)}
                child {node {E(3,5)}}};
    \end{tikzpicture}
    \caption{A valid proof tree for $T(1,5)$ in the program in \cref{prelim:Program}}
    \label{prelim:validTree}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.45\linewidth}
        \begin{tikzpicture}[, every node/.style={circle}]
            \node {T(1,7)}
                child {node {T(1,3)}
                    child {node {E(1,3)}}
                    }
                child {node {T(3,6)}
                    child {node {E(3,5)}}}
                child {node {E(6,7)}};
        \end{tikzpicture}
        \caption{An invalid tree for $T(1,7)$ in the program in \cref{prelim:Program}}
        \label{prelim:invalidTree}
    \end{subfigure}

    \caption{Examples of valid and not valid proof trees}
    \end{figure}

\begin{example}
    The tree in \cref{prelim:validTree} is valid. The leaves of this tree are facts and all other nodes represent ground rules from $ground(P)$ in the previous step.

    The tree in \cref{prelim:invalidTree} is not valid. $E(6,7)$ is neither a fact nor in the database. Additionally there is no rule in $P$ where applying some grounding yields $T(1,7) \leftarrow T(1,3), T(3,6), E(6,7)$.
\end{example}

The proof-theoretic semantics of a program $P$ and database $d$ is the set of ground atoms that have a valid proof tree for $P$ and $d$. Again this can be shown to be equal to the other definitions of semantics. We will formally prove the equality of the proof-theoretic and the model-theoretic semantics of datalog in this work.

In the rules of the program $P$ we note a special rule of the form $T(?x, ?x) \leftarrow$. It is a fact, but not a ground rule and the only rule where this is the case. 
We call a rule $H \leftarrow B_1, \dots , B_n$ \textit{safe}, if every variable that occurs in the head also occurs in the body, i.e. \[Vars(H) \subseteq \bigcup_{i \in \{1,..,n\} } Vars(B_i) \].

We call a program safe if every rule in the program is safe. Most works only consider safe programs as their ground program does not depend on the set of constants $C$ which may not be given in practice.

\begin{example}
    We can transform $P$ into the safe program $P'$ by adding a new unary predicate $N$ to the predicate symbols. $P'$ is then the union of $P \setminus \{T(?x, ?x) \leftarrow \}$ and the following rules:
    \begin{equation*}
        \begin{split}
            N(?x) &\leftarrow E(?x, ?y) \\
            N(?y) &\leftarrow E(?x, ?y) \\
            T(?x,?x) &\leftarrow N(?x) \\
        \end{split}
    \end{equation*}

    The new predicate $N$ (for nodes) represents any elements that occur in the $E$ relation and is used in the body for the reflexive rule.

    If some constants $c$ are desired that do not occur in any $E$ relation, one can also directly encode that into the database by adding $N(c)$.
\end{example}

What are the semantics of this newly created safe program $P'$? It turns out, that it is equal to the semantics for $P$ for all original predicates, i.e. if we remove all ground atoms that use $N$ we gain the same result. Therefore we state that any program can be transformed into an equivalent safe program.

\section{Certifying algorithms}

\begin{figure}
    \centering
    \begin{tikzpicture}[node distance=2.5cm and 20cm, auto]

        \tikzstyle{block} = [rectangle, minimum width=2cm, minimum height=1cm, text centered]
        
        \node [block] (box1) {Input};
        \node [block, right of=box1, draw] (box2) {Program};
        \node [block, right of=box2] (box3) {Output};
        
        \draw [->] (box1.east) -- (box2.west);
        \draw [->] (box2.east) -- (box3.west);
        
    \end{tikzpicture}
    \caption{A convential algorithm}
    \label{fig:programNormal}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}[node distance=2.5cm and 1cm, auto, scale=0.5]

        \tikzstyle{block} = [rectangle, minimum width=2cm, minimum height=1cm, text centered]
        
        \node [block] (box1) {Input};
        \node [block, right of=box1, draw] (box2) {Algorithm};
        \node [block, below of=box3] (box4) {Certificate};
        \node [block, right of=box2] (box3) {Result};
        \node [block, right of=box3, draw ] (box5) {Checker};
        \node [block, right of=box5, align=left] (box6) {Accept/\\Not accept};
        \node [block, above of= box5] (box7) {Output};

        \draw [->] (box1.east) -- (box2.west);
        \draw [->] (box2.south) |- (box4.west);
        \draw [->] (box2.east) -- (box3.west);
        \draw [->] (box3.east) -- (box5.west);
        \draw [->] (box4.east) -| (box5.south);
        \draw [->] (box5.east) -- (box6.west); 
        \draw [->] (box3.north) |- (box7.west); 
    \end{tikzpicture}
    \caption{A certifying algorithm}
    \label{fig:certifyingAlgorithm}
\end{figure}

Most algorithms one encounters either in books like \cite{AlgorithmsBook} or in real programs are comparable to \cref{fig:programNormal}. We have an input, give this into an algorithm which is a black box for us and then we receive a result as an output. Without inspecting the implementation we have to trust that the results are correct.
An alternative to this are \textit{Certifying algorithms} depicted in \cref{fig:certifyingAlgorithm}. They offer an explanation in addition to the result that can be checked independently by a checker or the user themselves. We follow the presentation in \cite{CertAlg} which offers information far beyond this section as well. 

Recall a definition of the class NP from complexity theory:

\begin{definition}[\cite{complexityBook}]
    A language $L \subseteq \{0,1\}^\star$ is in NP if there exists a polynomial $p: \mathbb{N} \to \mathbb{N}$ and a polynomial-time Turing machine $M$ such that forall $x \in \{0,1\}^\star$:

    \[ x \in L  \Leftrightarrow \exists u \in \{0,1\}^{p(|x|)}\text{ s.t. $M$ accepts $(x,u)$} \]

    If $x$ is in $L$, $u \in \{0,1\}^{p(|x|)}$ and $(x,u)$ is accepted by $M$, then we call $u$ a certificate for $x$
\end{definition}

We can generalize this and require an algorithm not only to give us an output $x$ but also a certificate $u$ as a reason why $x$ is the correct output. Then we can either check by ourselves that $x$ is a correct solution according to $u$ or use another program that checks this. As verifying a solution is never harder than computing it, the checker is usually simpler so that we either see directly that the checker is correct or we can formally verify it.

The formal framework of \cite{CertAlg} defines certifying algorithms in the following way. We consider the set $X$ of input values and the set $Y$ of output values of a function $f$. A predicate $\phi: X \to \{true, false\}$ states a precondition for the inputs and another predicate the postcondition $\psi: X \times Y \to \{true, false\}$, $\phi$ allows the algorithm to only accept part of the input space and $\psi(x,y)$ typically expresses that $y$ is a valid output for the input $x$. In cases where $x$ is not a valid input, we use the new symbol $\bot$ to denote that the algorithm does not return anything and use instead of $Y$ the following set of output values $Y^\bot = Y \cup \{\bot\}$.
The certificate or \textit{witness} from a set $W$ and its correctness is expressed by the predicate $\mathcal{W}: X \times Y^\bot \times W$ with the following properties:

\begin{enumerate}
    \item \textbf{Strong witness property}: Consider a triple $(x,y,w)$ that satisfies the witness predicate $\mathcal{W}$. If $y=\bot$, i.e. the input is not valid, we want $w$ to be a proof of this fact. Otherwise, we have that $y\in Y$ and we want $w$ to be a proof that the postcondition is satisfied. In total, we have the following requirements:
    \[ \forall x,y,w.\ (y = \bot \land \mathcal{W}(x,y,z) \rightarrow \neg \phi(x)) \land (y \in Y \land \mathcal{W}(x,y,z) \rightarrow \psi(x,y)) \]
    \item \textbf{Simplicity}: The strong witness property has a simple proof
    \item \textbf{Checkability}: It is possible to check efficiently if $\mathcal{W}(x,y,w)$ holds for a triple $(x,y,w)$
\end{enumerate}

A \textit{(strongly) certifying algorithm} is an algorithm that stops on all inputs $x \in X$ and returns a tuple $\langle y, w\rangle$ such that $\mathcal{W}(x,y,w)$ holds.

This is illustrated in the following examples. We start with an example from graph theory.

\begin{example}[\cite{CertAlg}]
    Consider the problem of deciding whether a graph $G=(V;E)$ is bipartite, i.e. there exists a partition of $V$ into $V_1, V_2$ with $V = V_1 \cup V_2$ and $V_1 \cap V_2 = \emptyset$, such that for all edges $e \in E$ we have that $e \cap V_1 \neq \emptyset$ and $e \cap V_2 \neq \emptyset$, so that all edges are only between vertices that are in the different partitions.

    The set of inputs $X$ is the set of strings over $\{0,1\}$ and $\phi(x)$ holds whenever $x$ encodes a graph. 
    The postcondition is the following: If $y= true$, then $x$ encodes a bipartite graph. If $y = false$, then $x$ does not encode a bipartite graph.

    Adopting the well-known algorithm for checking whether a bipartite graph, we can construct the following certifying algorithm.

    \begin{enumerate}
        \item Check if $x$ encodes a binary graph. If not return $\bot$ and a witness that describes the problem in the encoding.
        \item Explore the graph in depth-first search and color vertices along the path by alternating two colors. If we want to color a vertex by a color $c$ and it already has the color $c'$, we continue exploring another path if $c=c'$, or stop if $c \neq c'$ and return $false$. If $c \neq c'$ is the case, we have found a cycle of odd length in our current path and return this cycle as a witness.
        \item If we reach this step, then all vertices are colored in a way that no neighboring vertices have the same color. Then we return $true$ and the two sets of colored vertices as the witness. 
    \end{enumerate}

    We know from graph theory that a graph is bipartite iff it has no cycle of odd length.

    For any value of $y$ a checker can efficiently check if the returned reason does indeed hold for $x$.
\end{example}

The above example illustrates the concept well for a decision problem, but we want to verify whether a set of ground atoms is the semantics of a datalog program and database, i.e. a function problem. The next example illustrates a certifying algorithm for a function problem in number theory.

\begin{example}[\cite{CertAlg}]
    Let $X$ be the set of pairs of natural numbers and $Y$ the set of natural numbers. We want to compute the greatest common divisor, $gcd$, for two numbers $a$ and $b$ that are not equal to zero, i.e. the largest $g \in \mathbb{N}$ that is a divisor of both $a$ and $b$. The precondition $\phi(\langle a, b\rangle)$ is that both $a$ and $b$ are different from zero and the post-condition is that $\psi(\langle a, b\rangle, g)$ shall be true if $g$ is the greatest common divisor of $a$ and $b$.

    The normal way of computing this involves the Euclidean algorithm, but this is not a certifying algorithm. The extended Euclidean algorithm however returns in addition to the greatest common divisor $g$ two integers $s$ and $t$ such that $g = gcd(a,b) = a s + b t$. In \cite{CertAlg}, the back direction is proven, i.e. that if $g$ is a divisor of $a$ and $b$ and $g = a s + b  t$ holds for two integers $s$ and $t$, then $g$ is also the greatest common divisor of $a$ and $b$.

    Therefore a certifying algorithm to compute the $gcd$ is the extended Euclidean algorithm, that returns as $y$ $gcd(a,b)$ and as the certificate the pair $\langle s, t\rangle$.

    A checker then only has to check that $g=a s + b t$ holds and that $g$ divides both $a$ and $b$. 
\end{example}

For decision problems, we require an explanation when the input is in a language as well as an explanation when the input is not in the language. A common misconception is that therefore certifying algorithms are only possible for problems that are known to be in $NP \cap coNP$, which is presumed to not include the interesting $NP$-complete problems. This is not the case as we do not require the certificate to be polynomial in the size of the input. Indeed, SAT-solvers that decide the NP-complete satisfiability problem of propositional logic offer certificates in the DRAT proof format as a model or a proof of unsatisfiability\cite{DRAT}.

In the datalog case, many modern reasoners such as Nemo or Soufflé already offer certificates for atoms in the form of the previously introduced proof trees. We therefore focus on implementing a checker for datalog.

In \cite{CertCheckerWorkflow} an approach to develop checkers for certifying algorithms is outlined. There the checker is implemented in C because already the certifying algorithm itself is written in C++, so that both versions are similar. In order to verify the correctness of the checker they employ the proof assistant Isabelle. The C-code is exported by a tool into Isabelle so that the verification can take place. In Isabelle then the problem is defined and it is proven that the export does fulfill the desired properties. 

In this workflow, we have to trust the correctness of the hardware, the C compiler, the tool that exports C code to Isabelle and Isabelle itself. Additionally, we have to trust that we have correctly defined and specified the problem in Isabelle.

In this work, we modify this workflow a bit. We use a different proof assistant, Lean, instead of Isabelle. Lean is not only a proof assistant but also a functional programming language. Therefore we can implement the checker directly in Lean and do not have to export a model of our code into the proof assistant. Our trust base is therefore smaller, as we no longer need an additional compiler for the language the checker is written in nor a tool to export this into the proof assistant. 


\section{Lean}
In this section, we introduce Lean, a programming language and proof assistant. As of writing, the current version is Lean 4. A more in-depth introduction can be found in \cite{theoremProvingLean} or \cite{functionalProgrammingLean}.

Lean's core is a small trusted kernel\cite{LeanSysDescr}, that captures the most important functionalities and can be extended by the user. Since version 4 Lean is implemented in Lean itself\cite{Lean4}. The most important libraries for Lean that we will use are the standard library Std4\cite{stdLean} which contains important data structures and tactics and mathlib4\cite{mathlib}, which contains definitions, tactics and theorem from diverse areas of mathematics and computer science.

Lean can encode many mathematical results. The foundations of mathematics are often built on top of set theory (e.g. \cite{logic}) but most proof assistants instead use \textit{type theory}. Type theory was introduced by Bertrand Russel as an alternative foundation during the foundational crisis of mathematics.

Any element, a \textit{term}, has a \textit{type} in type theory. This is denoted by \lstinline|term:type|. 

\begin{example}
    Different terms and their types are displayed below:
        \begin{lstlisting}
            42:nat
            true:bool
            sort([1,2,3]): List (nat)
            nat: Type
            Type: Type₁
        \end{lstlisting}

        \lstinline|nat| is a natural number, \lstinline|true| is a boolean, the result of the application of \lstinline|sort| to \lstinline|[1,2,3]| has the type \lstinline|List nat|. All previous types have a type as well, \lstinline|Type|. \lstinline|Type| itself also has a type \lstinline|Type₁|. This forms an infinite sequence of \textit{type universes}, that are non-cumulative, i.e. no term has multiple types.
\end{example}


In contrast to set theory, functions are a direct element of type theory and do not need a complicated encoding. For any types \lstinline|A| and \lstinline|B|, there exists the type of functions from A to B, \lstinline| A $\to$ B|.

We can define functions in Lean by the keyword \lstinline|def| similar to other functional programming languages, such as the id function for natural numbers. In the parentheses, we denote an argument \lstinline|x| with its type. Due to currying a function can have multiple arguments. After the colon, we denote the returned type of the function, which is here again a natural number. Therefore we have the following type \lstinline|$\mathbb{N} \to \mathbb{N}$|, as the function takes a natural number as an input and returns a natural number as well. After the \lstinline|:=| we denote the natural number this function returns. 
\begin{lstlisting}
            def id (x: $\mathbb{N}$): $\mathbb{N}$:= x
\end{lstlisting}

Everything we have seen so far holds in general in type theory. Lean itself is based however on a specific type theory, the Calculus of Inductive Constructions (CIC), a dependent type theory\cite{Lean4, CoC}.

The id function we defined above is correct but only works for natural numbers. We would have to implement another id function for booleans, string or any other type. This is rather cumbersome because we never actually use any properties of the specific type. We can generalize the id function to help us with this problem.

\begin{lstlisting}
    def id (A: Type)(a: A): A:= a
\end{lstlisting}

This id function has now two arguments. The first argument is the type we want to use it on, the second argument is an element of this type. \lstinline| id $\mathbb{N}$| works as previously and has the type \lstinline|$\mathbb{N} \to \mathbb{N}$|. It seems less clear which type id itself has because this depends on the first argument.
Such types are called \textit{dependent types} and we would denote this specific type as \lstinline|$\Pi$ (A:Type) A $\to$ A|, known as a \textit{Pi type}. Intuitively, a Pi type is a function that returns a type for any input value.

In practice, this is often written in the following way:
\begin{lstlisting}
    def id {A: Type}(a: A): A:= a
\end{lstlisting}

Here the type $A$ is an implicit argument, which is denoted by the braces, and Lean can fill it in for the user based on the input $a$. Therefore the user can write just \lstinline|id 42| instead of \lstinline|id $\mathbb{N}$ 42|.

So far we have discussed what types are and how we can define functions that are important for the programming aspect of Lean, but this does not tell us much about the use as a proof assistant. Due to the Curry-Howard correspondence a proof of a statement can be equivalently seen as a function that has the type of the statement. The type of statements is known in Lean as \lstinline|Prop|. There exist functions like \lstinline|Or: Prop $\to$ Prop $\to$ Prop| and the dependent types form the quantifiers. We view the Pi-type $\Pi (a:A), \beta(a)$ as a function mapping $a$ to the true statements of $\beta(a)$, which is equivalent to the universal quantor. Prop therefore has elements such as \lstinline|$\forall (x:\mathbb{N}$), 0 $\le$ x | or \lstinline|$\exists$ (x y: Bool), x = y $\lor$ $\neg$ x = y|.

Every statement defines its own type, whose elements are the proofs for it. In contrast to other types, all elements of these types are equivalent which is known as \textit{proof irrelevance}, so that elements of these types only serve as a witness for their truth.

Proofs can be constructed in Lean in two ways.  We start by writing \lstinline|theorem| (or lemma, proposition etc. in mathlib) and denote the statement as we did for a function. The main difference is that the type is what we want to prove and that the function can not be executed. An example of a statement and its proof in Lean is shown below:

\begin{lstlisting}
theorem testTheorem {A: Type}: ∀ (x y z: A), x = y → id y = z → z = x := by
  intro x y z h1 h2
  unfold id at h2
  rw [h1]
  apply Eq.symm
  exact h2
\end{lstlisting}

The proof is done in tactic mode. We open the tactic mode with the keyword \lstinline|by|. At the start our context only contains that $A$ is a type and the goal is $∀ (x\ y\ z\: A), x = y → id\ y = z → z = x$, which is compactly written as

\[ \{A:Type\} \vdash ∀ (x\ y\ z: A), x = y → id\ y = z → z = x\]

We apply tactics to change the goal or a hypothesis in the context. These may introduce new goals or fulfill old goals. A proof is finished, when all goals are proven. We explain the proof now in more detail.

\begin{enumerate}
    \item We start with the \lstinline|intro| tactic. This tactic moves elements that are universally quantified or the start of an implication from the goal into the context. Alternatively, one can use \lstinline|revert| to move a hypothesis from the context back to the goal to prove a stronger statement.
    
    At the end, the state is: 

    \[ \{A:Type\} (x\ y\ z: A) (h1: x = y) (h2: id\ y = z)\vdash z = x\]

    \item After that we use the \lstinline|unfold| tactic at the hypothesis h2. This tactic replaces \lstinline|id| with its definition in h2, which now looks like this: $(h2: y = z)$
    
    \item We use now the tactic \lstinline|rw|, i.e. rewrite, that replaces in the goal $x$ by $y$ due to the hypothesis $h1$ so that the goal is now $z = y$.
    \item The goal is now almost the same as $h2$, but the order is wrong. Therefore we use the apply tactic with
    
    \lstinline|Eq.symm := $\forall \{A:Type\} (a b:A), a = b \rightarrow b = a$ |. 

    Apply tries to match the statements consequent with our goal and opens new goals for the antecedents of this statement.

    Our current proof state is then this:

    \[ \{A:Type\} (x\ y\ z: A) (h1: x = y) (h2: y = z)\vdash y = z\]

    \item Our goal is now exactly the same as the hypothesis $h2$. The \lstinline|exact| tactic uses $h2$ to complete the goal.
\end{enumerate}

Other important tactics not seen here are \lstinline|by_contra| which allows a proof by contradiction, \lstinline|by_cases| which allows a case distinction or \lstinline|constructor| that splits an and statement into two goals. Lean offers also methods that try to prove goals themselves like \lstinline|simp| that uses lemmas marked with \lstinline|@simp| or \lstinline|tauto|, that can recognize some tautologies. The set of tactics is however not fixed and the user can introduce new tactics to simplify the reasoning process.

The majority of formal proofs in this work are done in tactics mode, but we see above that describing them in natural language is quite verbose. Therefore we will only describe the idea we followed in the following sections. Any result is also linked to the code in the later sections and can be directly accessed via the hyperlink to see the formalization instead.

These proofs are done backwards, i.e. starting from the goal towards the assumptions. There are also forward proofs that start from the assumptions by building hypotheses until we reach the goal or by composing theorems like functions. While this is possible, it was not done in this work. A good source for this is either \cite{theoremProvingLean} or \cite{HitchhikerLogicVer}.


We can also use \lstinline|def| to define new types. Sets of type \lstinline|A| are in Lean implemented as functions from \lstinline|A| to \lstinline|Prop|. Functions to \lstinline|Prop| are in general not computable so that the membership in a set is also not computable in contrast to membership in lists.

\begin{lstlisting}
    def Set (α : Type) := α → Prop
\end{lstlisting}

This introduces a new type. If we want to just use functions that accept certain elements of type \lstinline|Set|, using \lstinline|def| would create a new type for which we would have to define again the set operations $\cup$ or $\cap$. In this case, one can use the \lstinline|abbrev| command which provides a simple alias.

\begin{lstlisting}
    abbrev NatSet := Set $\mathbb{N}$
\end{lstlisting}

The CIC and Lean allow the creation of types by induction. We can create our own implementation of the natural numbers with the \lstinline|inductive| keyword. After that, we list the constructors. These can be constant constructors such as \lstinline|zero| or function constructors like \lstinline|succ|.
An inductive type may have many constructors of both types or even none as the empty type.

\begin{lstlisting}
    inductive myNat: Type
    | zero: myNat
    | succ: myNat → myNat
\end{lstlisting}

Our definition states that an element of \lstinline|myNat| is either \lstinline|zero| or the result of applying \lstinline|succ| to some other element of \lstinline|myNat|. Any element of \lstinline|myNat| is only the result of one of these constructors and all elements of \lstinline|myNat| are distinct.

A typical mathematical definition for this would be: myNat is the smallest set $S$ of elements that contains $zero$ and whenever an element $a$ is in $S$, then also $succ\ a$ is in $S$.

We can define for inductive types functions recursively like the add function. Here we reuse the inductive schema. If $m$ is zero, then $n+m$ is simply n. If $m$ instead is the successor of some other myNat $m'$, then we return the successor of \lstinline|add n m'|.

\begin{lstlisting}
    def myNat.add (n m: myNat): myNat :=
    match m with
    | zero => n
    | succ m' => succ (add n m')
\end{lstlisting}

If we want to prove a statement about elements on an inductive type, we can use the \lstinline|induction| tactic to create proofs similar to those by (structural) induction in natural language proofs. If no induction hypothesis is needed one can also use the \lstinline|cases| tactic that creates a goal for every constructor. We illustrate the induction tactic by proving that adding to zero any myNat $n$ yields $n$.

\begin{lstlisting}
            theorem myNat.add_zero (n: myNat): myNat.add zero n = n := by
                induction n with
                | zero =>
                    unfold add
                    rfl
                | succ n' ih =>
                    unfold add
                    rw [ih]
\end{lstlisting}

We start with the induction tactic to perform an induction over n. Each constructor is listed separately and after the arrow, we start the proof for this constructor. In the zero case, it is enough to unfold the definition of add to get \lstinline|zero=zero| and use \lstinline|rfl| which proves these equalities.

In the succ case, we have two values:  the myNat element $n'$ we use in the constructor of succ and also the induction hypothesis\lstinline|ih: add zero n' = n'|. We can unfold the definition of add again and use \lstinline|rw| to replace \lstinline| add zero n'| by \lstinline|n'| in the goal \lstinline|succ (add zero n') = succ n'| due to the induction hypothesis \lstinline|ih|. The \lstinline|rw| tactic applies \lstinline|rfl| at the end automatically to finish the proof.

After defining the natural numbers inductively, we can also define the even numbers by induction. We again have the zero constructor and a succ constructor that now shall represent the next natural number.

\begin{lstlisting}
    inductive evenNat: Type
    | zero: evenNat
    | evenSucc: evenNat → evenNat
\end{lstlisting}

While this works, we see no relation between the elements of myNat and evenNat. We can of course define a function that maps elements of evenNat to myNat and use this to transform any even number to a natural number.

\begin{lstlisting}
    def evenNatToMyNat (e: evenNat) := 
        match e with
        | zero => myNat.zero
        | succ e' => myNat.succ (myNat.succ (evennatToMyNat e'))
\end{lstlisting}

This approach works but is a bit cumbersome. We always need to type this ourselves and if we forget it Lean raises a type error. There exists however the \lstinline|class Coe|. A class describes an abstract property e.g. of a type and can be implemented by a specific type via \lstinline|instance|.

The class \lstinline|Coe| is defined (in a simplified version) as follows. It takes two types and a function that maps from one type to the other. We implement \lstinline|COe| for \lstinline|evenNat| and \lstinline|myNat| with \lstinline|evenNatToMyNat|. Then we can use elements of \lstinline|evenNat| in functions defined for \lstinline|myNat| and Lean will automatically convert them using \lstinline|evenNatToMyNat|.

\begin{lstlisting}
class Coe (α : Type) (β : Type) where
    coe : α → β

instance coeEvenNatMyNat: Coe evenNat myNat := ⟨evenNatToMyNat⟩
\end{lstlisting}

An alternative way of defining types are structures that are comparable to classes in object-oriented languages. Structures have fields that have a type and we start listing them one by one after the \lstinline|where| statement. Structures are like inductive types with a single constructor. Therefore they are not allowed to be recursive, i.e. no structure $A$ may have an element of type $A$, because there would be no base case.
We can build an element of the type of the structure by passing the elements in braces and can use the name of the field to access an element.

\begin{lstlisting}
structure player where
    (name:String)
    (numGoals: $\mathbb{N}$)
    (active: Bool)

def player1: player := {
        name := "Test",
        numGoals := 10,
        active := true
    }

    def player1Goals: $\mathbb{N}$ := player1.numGoals
\end{lstlisting}

Returning to our example of myNat, we can define some more functions. In the previous section, we talked about the Euclidean algorithm. We can try to implement it ourselves in Lean. Firstly, we would require the modulo function for that. We will only declare the function for now to not lose focus and use the keyword \lstinline|sorry| in the value. This closes in proofs any goal or can be used for incomplete functions, but will throw an error if executed.

\begin{lstlisting}
    def myNat.mod (n m: myNat): myNat := sorry
\end{lstlisting}

The Euclidean algorithm can then be implemented in the following way:

\begin{lstlisting}
    def myNat.Euclid (a b: myNat): myNat :=
    if b = zero
    then a
    else Euclid b (myNat.mod a b)
\end{lstlisting}

Lean will not accept this function and instead raise an error in the line of the if that it failed to synthesize an instance of \lstinline|DecidableEq| for myNat. DecidableEq allows us to use $=$ between elements of a type in a function. This is not a trivial requirement as the equality of real numbers is undecidable\cite{EqualityRealNumber}. We need to provide a function to Lean that can be used to decide whether two elements of myNat are equal. During the definition of inductive types, we already noted, that all elements of an inductive type are different, which allows us to do this. Luckily, Lean can even do this alone if we tell Lean to do this using the \lstinline|deriving| keyword. It can even derive instances for multiple classes. Another important class would be the Inhabited class which states that there exists some default element of this type.

\begin{lstlisting}
    inductive myNat: Type
    | zero: myNat
    | succ: myNat → myNat
    deriving DecidableEq, Inhabited

\end{lstlisting}

Now the error will disappear, but a new one will be introduced. Lean does not recognize the termination of \lstinline|myNat.Euclid|. We have already defined recursive functions such as \lstinline|myNat.add| before and we did not encounter this problem. In \lstinline|myNat.add| we however used the inductive schema and only did the recursion on $n$ in the case of $succ\ n$. This allowed Lean to conclude that we only call the function on smaller elements so that the function will terminate.

To show termination, one is required to define some \textit{well-founded relation}, i.e. a relation that does not have any infinite descending chains. Such a relation would be the less-than relation of the natural numbers so it is often convenient to define some size measure of the elements of a type. Such a function exists automatically for any inductive type and is called \lstinline|sizeOf|. We can provide this and are only left to prove that \lstinline|sizeOf (mod a b) < sizeOf b|, which we will leave open here.

\begin{lstlisting}
def myNat.Euclid (a b: myNat): myNat :=
  if b = zero
  then a
  else Euclid b (myNat.mod a b)
termination_by sizeOf b
decreasing_by
  simp_wf
  sorry
\end{lstlisting}

An alternative to this termination proof would be to mark this function using the \lstinline|partial| keyword, which denotes that this function does not always terminate. This is however often not beneficial as we no longer can use the \lstinline|unfold| tactic to gain the definition of this function in a proof.



We want to take a look at another example of an inductive type, that we will often use namely lists. A list (here List' as List is already a standard definition) is either the empty list or the result of combining a list with a new element and we can define a get function

\begin{lstlisting}
    inductive List' (A: Type): Type
    | nil : List' A
    | cons : A → List' A → List' A
    deriving DecidableEq

    def getFirstElementOrDefault (A: Type) [Inhabited A] (l: List' A): A :=
    match l with
    | nil => Inhabited.default
    | cons hd _ => hd
\end{lstlisting}

We see in this definition a new type of argument. We want to return an element of the list. If the list is empty, this proves to be difficult. In that case, we return the default element that exists because $A$ is an instance of \lstinline|Inhabited|. Such a type-class parameter is given in brackets and Lean will look for the instance that shows the implementation of the type class for this type when using this function.

This may not always be the desired behaviour and maybe we want to return nothing if the list is empty. In this case, we can use the \lstinline|Option| Type. It has two constructors, one contains the element and the other symbolizes that there is nothing. Here $\alpha$ has the type \lstinline|Type u| that symbolizes it works for types from any type universe. Then we can return none if the list is empty and else return the same value as before.

\begin{lstlisting}
    inductive Option (α : Type u) where
    | none : Option α
    | some (val : α) : Option α

    def getFirstElement (A: Type) (l: List' A): Option A :=
    match l with
    | nil => Option.none
    | cons hd _ => Option.some hd
\end{lstlisting}

We will also often use the Exception type. This uses two types $\epsilon$ and $\alpha$. The type $\epsilon$ is the error we report and $\alpha$ are the elements we return if no error occurs.


\begin{lstlisting}
    inductive Except (ε : Type u) (α : Type v) where
  | error : ε → Except ε α
  | ok    : α → Except ε α

  def getFirstElement (A: Type) (l: List' A): Except String A :=
    match l with
    | nil => Except.error "Called getFirstElement on emptyList"
    | cons hd _ => Except.ok hd
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Lean_example2.png}
    \caption{An example of Lean in practice. On the left side is the code seen and on the right side the context in a proof with the current goal is displayed}
\end{figure}
